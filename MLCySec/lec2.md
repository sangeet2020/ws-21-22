# Lecture 2

## Goals
- What is ML?
- assumptions under ML
- design choices
- practical considerations

## ML
- learning from data
- large amount of data
- high computational resource for prameter estimation in large models
- advances in ML algorithm
- **Terminlogies**
    - training
    - testing
    - generalization: classifier predicts well on unseen data.
    - model: prametrized function class in which the classifier is chosen
- **Inference**
    - Inductive inference: learning general principles from observation
        - can only be falsified, but not verified
    - Deductive inference: deriving specific assertions from general principles
- ML tries to automate inductive inference.
- Hypothesis: several functions coming to same conclusion

## Types of learning
- Supervised <br>
Given $n$ observations
    $$
    T = (X_i, Y_i)^n_{i=1}
    $$
- semi-supervised
    - clustering
    - density estimation:  estimating distribution of the input points over the input space $\mathbf{X}$. (Outlier detection)
    - dimensionality reduction: creating a mapping of the target space in much smaller space than that of the input space. The mapping should preserve properties of input space.
- unsupervised

## Batch, online, active learning
- Batch
    - all training data at once
- online
    - training data arrive sequentially, leading to sequential updates
- active learning
    - variant of semi-supervised learning (not very important)

## Statiscal learning
- Assumption: data is generated by sampling from a probablity measure.
- Discriminative vs generative learning

## Challenges
- choice of features
- integration of prior knowledge
- computational complexity
- curse of dimensionalality
- over and under-fitting

## Curse of dimensionality
- with lots of features in the input data, the input or the feature space becomes of high dimension.
- Naive histogram estimator
    - $X = [0, 1]^d$
    - there are $k^d$ different bins
    - classify each bin with the label occurng most often in that bin (majority vote)

## Bias variance tradeoff
- check slide 21
- Model error and estimation error

## Purpose of different ML algo
- no universal best method
- on an avg they all perform similar
- each learning method models prior knowledge
- data can never replace prior knowledge

## Loss function and risk
- Loss function
    - mapping from two label into true mapping
    - Examples: classification (0-1 loss), regression (squared looss)
- Risk (or expected loss)
    - Bayes optimal learning rule minimized expected risk


## Emperical risk minimization
- check slide 26,27



